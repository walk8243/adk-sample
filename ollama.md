# Ollamaを使ってローカルPCでGemma 3を実行する方法

ローカルPC環境で大規模言語モデル（LLM）を実行するためのツール「Ollama」を導入し、Googleの軽量モデル「Gemma 3」を動作させるまでの手順を解説します。

## 1\. ローカルLLMとOllamaについて

### ローカルLLMとは？

通常、ChatGPTやGeminiなどの高性能なAIは、クラウド上の強力なサーバーで動作しています。ローカルLLMは、個人のPC（Windows, macOS, Linux）上で直接動作するように設計された、比較的小規模な言語モデルです。インターネット接続なしで利用できる利点があります。

### Ollama（オラマ）とは？

Ollamaは、これらのローカルLLMを非常に簡単にセットアップし、実行するためのツールです。

  * **簡単なセットアップ**: コマンドライン（ターミナル）から数行のコマンドを実行するだけで、モデルのダウンロードと実行が完了します。
  * **多様なモデルに対応**: Llama 3, Phi-3, Mistral、そしてここで紹介するGemmaなど、多くの人気モデルに対応しています。

## 2\. Ollamaのインストール

まず、Ollamaをお使いのPCにインストールします。

1.  Ollamaの公式サイトにアクセスします。
      * [https://ollama.com/](https://ollama.com/)
2.  お使いのOS（macOS, Windows, Linux）用のインストーラーをダウンロードし、画面の指示に従ってインストールを完了させます。

## 3\. 実行するGemma 3モデルの確認

Ollamaでは、`gemma`（旧バージョン）や `gemma3`（新バージョン）など、様々なモデルが利用可能です。
どのモデル（タグ）が利用できるかは、Ollamaの公式ライブラリ（[Ollama Hub](https://ollama.com/search)）で確認するのが確実です。

1.  Ollama HubのGemma 3のページにアクセスします。
      * [https://ollama.com/library/gemma3](https://ollama.com/library/gemma3)
2.  「**Tags**」タブをクリックすると、利用可能な全バージョン（例: `270m`, `1b`, `4b`, `27b`, `latest` など）の一覧が表示されます。

## 4\. Gemma 3 (270Mモデル) の実行

今回は、Gemma 3ファミリーの中で最も軽量で高速な「270M (2億7000万パラメータ)」モデルを実行してみます。

1.  **ターミナルを開く**

      * macOS: 「ターミナル.app」
      * Windows: 「コマンドプロンプト」または「PowerShell」

2.  **実行コマンドの入力**
    以下のコマンドを入力し、Enterキーを押します。

    ```bash
    ollama run gemma3:270m
    ```

      * **初回実行時**: Ollamaが自動的に `gemma3:270m` モデルのダウンロードを開始します。
      * **ダウンロード後**: モデルの読み込みが完了すると、プロンプト（入力待機状態）が表示され、チャットが可能になります。

### (参考) 他のGemma 3モデルを実行する場合

もしPCのスペック（特にRAMメモリ）に余裕がある場合は、より高性能なモデルも試すことができます。

  * **4B (40億) モデル**:
    ```bash
    ollama run gemma3:4b
    ```
  * **1B (10億) モデル**:
    ```bash
    ollama run gemma3:1b
    ```

## 5\. 動作確認（最初のプロンプト）

モデルが起動し、入力待機状態になったら、動作確認のために簡単な質問をしてみましょう。
`gemma3:270m` は非常に軽量なため、応答が速いのが特徴です。

**入力例:**

```text
こんにちは。あなたは誰ですか？
```

```text
日本の首都はどこですか？
```

```text
猫について短い詩を書いてください。
```

これで、ローカルPC上でGemma 3が動作する環境が整いました。
